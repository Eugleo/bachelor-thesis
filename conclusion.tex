\chapwithtoc{Conclusion}

In this thesis we have reviewed the current approaches to disulfide bond mapping, and the different tradeoffs stemming from various choices regarding instrumentation, data preparation, and experiment design. We presented our own attempt at automatic computational disulfide bond characterisation, a program aiming to identify even complicated clusters of disulfide bonds, and intrapeptide disulfide bonds.

During the evaluation of the algorithm on real-world data we have inadvertently confirmed the occurence of DB scrambling during tryptic protein digestion. Furthermore, due to the high number of false positives among the assigned fragments, and the rather unsophisticated method of visualisation, a fair amount of manual labour is still needed to interpret the results. Another consequence of this is that the scoring metric needs to be quite complicated and opaque in order to weed out the false positve matches. Further research is needed to simplify the scoring metric --- a part of the solution will probably be a better utilisation of the data from RAT samples.

Despite these limitations, and the challenging conditions regarding the data we used, we have successfully demostrated the power of the program by identifying DBs in lysosyme, including one intra-peptide DB, and in our in-silico generated control dataset. We conclude that we have reached our goal of devising a general algorithm that is able to identify complicated crosslinked peptide fragments, but we consider it to be only a proof-of-concept at this stage of the development. Nonetheless, we think the approach of extensive matching of complicated peptide framgens could prove to be useful in mapping DBs in the future.

\section*{Future work}

It would be interesting to see how well the algorithm fares with data that are not digested with trypsin, to confirm whether the high number of unexpected identifications were due to the issue of DB scrambling during tryptic digestion. On this front, many other proteases would be viable, for example pepsin, or thermotrypsin~\cite{sung2016evaluation}. After a change in the sample preparation protocol, we expect the number of high-scoring ``bad'' variants to decrease, resulting in a stronger signal for the correctly identified DBs, and in a reduced number of false positives. The same outcome could also be achieved by optimising the scoring metric, and by more resourceful utilisation of the data from RAT samples. Last but not least, data from other fragmentation sources could be used to make the dataset richer.

Further work could be done to optimise Dibbi's performance. Divide and conquer algorithms lend themselves nicely to dynamic programming approaches, but the multitudes of information that the algorithm has to keep track of in our specific case --- such as neutral losses, amino acid modifications, cysteine alkylations, bond cleavages, error boundaries --- made it complicated to employ them. Nonetheless, should the program be deployed in real-life scenarios in the future, reductions in compute time would be needed. Additional speed-ups could be achieved by implementing the program in Julia\footnote{\url{https://julialang.org}} instead of Python.

Finally, due to its nature, Dibbi has very high sensitivity; so high in fact that it is almost detrimental due to the quantity of false positives, were it not for the elaborate scoring and weighting post-processing steps. However, Dibbi could be used to automatically label and analyse the various types of complex ions that show up in the fragmentation spectra, such as internal ions comprising of multiple crosslinked peptides. If the sample were prepared very carefuly, and the positions of DBs in the protein were known, this knowledge could be used to manually discard most of the false positives. In this way, Dibbi could be used to research and quantify the dissociation pathways of crosslinked peptides, potentially leading to more informed approaches to DB mapping in the future.